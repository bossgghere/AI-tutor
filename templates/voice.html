<!doctype html>
<html>
<head>
    <meta charset="utf-8" />
    <title>Zyvora ‚Äî Voice Mode</title>
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/marked@12.0.2/marked.min.js"></script>
    <script src="https://unpkg.com/three@0.158.0/build/three.min.js"></script>
    <style>
        @keyframes pulse {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(1.1); }
        }
        .mic-pulse {
            animation: pulse 1s infinite;
        }
        canvas.three-bg {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            z-index: 0;
        }
    </style>
</head>
<body class="bg-gradient-to-br from-black via-purple-900 to-blue-900 text-white min-h-screen flex items-center justify-center relative overflow-hidden">

    <!-- 3D Background Canvas -->
    <canvas id="three-bg" class="three-bg"></canvas>

    <div class="w-full max-w-2xl mx-auto p-6 relative z-10">
        <div class="bg-white/5 rounded-2xl p-6 shadow-xl backdrop-blur-md">
            <div class="flex flex-col items-center">
                <!-- Floating Mic Button -->
                <div class="relative w-32 h-32 flex items-center justify-center mb-6">
                    <button id="micContainer" class="rounded-full p-8 bg-gradient-to-r from-pink-500 to-blue-500 shadow-lg hover:scale-110 transition-transform duration-300">
                        <svg id="micIcon" class="w-16 h-16 text-white" xmlns="http://www.w3.org/2000/svg" fill="currentColor" viewBox="0 0 16 16">
                            <path d="M5 3a3 3 0 0 1 6 0v5a3 3 0 0 1-6 0z"/>
                            <path d="M3.5 6.5A.5.5 0 0 1 4 7v1a4 4 0 0 0 8 0V7a.5.5 0 0 1 1 0v1A5 5 0 0 1 8 13.5V15h3a.5.5 0 0 1 0 1H5a.5.5 0 0 1 0-1h3v-1.5A5 5 0 0 1 3.5 8V7a.5.5 0 0 1 .5-.5"/>
                            <path d="M10 8a2 2 0 1 1-4 0v1h4z"/>
                        </svg>
                    </button>
                </div>

                <canvas id="visualizer" class="w-full h-24 mb-4 rounded-xl bg-gray-900/40 shadow-inner"></canvas>
            </div>
            
            <div id="chatBox" class="h-56 overflow-auto mb-4 p-3 rounded-lg bg-gray-900/60 shadow-inner">
                <div id="initial" class="text-gray-200">I am ready. Try saying "Hello" to start.</div>
            </div>

            <div class="flex gap-3">
                <input id="manualInput" class="flex-grow rounded-full px-4 py-2 bg-gray-700 text-white outline-none" placeholder="Type to speak (optional)"/>
                <button id="sendManual" class="bg-green-500 px-4 py-2 rounded-full text-white font-semibold">Send</button>
            </div>
        </div>
    </div>

    <!-- THREE.JS BACKGROUND -->
    <script>
        const scene = new THREE.Scene();
        const camera = new THREE.PerspectiveCamera(75, window.innerWidth/window.innerHeight, 0.1, 1000);
        const renderer = new THREE.WebGLRenderer({ canvas: document.getElementById("three-bg"), alpha: true });
        renderer.setSize(window.innerWidth, window.innerHeight);

        const geometry = new THREE.SphereGeometry(2, 64, 64);
        const material = new THREE.MeshStandardMaterial({
            color: 0x00e0ff,
            transparent: true,
            opacity: 0.2,
            emissive: 0x00e0ff,
            emissiveIntensity: 0.6,
            wireframe: true
        });
        const sphere = new THREE.Mesh(geometry, material);
        scene.add(sphere);

        const light = new THREE.PointLight(0xffffff, 1, 100);
        light.position.set(10, 10, 10);
        scene.add(light);
        scene.add(new THREE.AmbientLight(0x404040));

        camera.position.z = 5;

        function animate() {
            requestAnimationFrame(animate);
            sphere.scale.setScalar(1 + Math.sin(Date.now() * 0.003) * 0.1);
            renderer.render(scene, camera);
        }
        animate();

        window.addEventListener("resize", () => {
            camera.aspect = window.innerWidth / window.innerHeight;
            camera.updateProjectionMatrix();
            renderer.setSize(window.innerWidth, window.innerHeight);
        });
    </script>

    <!-- VOICE + CHAT LOGIC -->
    <script>
        const userIdKey = "zyvora_user_id";
        function getUserId(){
            let id = localStorage.getItem(userIdKey);
            if(!id){
                id = "user_" + Math.random().toString(36).slice(2,9);
                localStorage.setItem(userIdKey, id);
            }
            return id;
        }

        function appendChatLine(who, text){
            const box = document.getElementById("chatBox");
            const el = document.createElement("div");
            el.className = who === "bot" ? "text-left text-white/90 mb-2" : "text-right text-white/80 mb-2";
            const formattedText = marked.parse(text);
            el.innerHTML = (who === "bot" ? "<strong>Zyvora:</strong> " : "<strong>You:</strong> ") + formattedText;
            box.appendChild(el);
            box.scrollTop = box.scrollHeight;
        }

        let isSpeaking = false;
        function speakText(text, lang = 'en') {
            if (!("speechSynthesis" in window)) return;
            stopSpeaking();
            const utterance = new SpeechSynthesisUtterance(text);
            utterance.lang = lang;
            utterance.onstart = () => { 
                isSpeaking = true; 
                toggleVisualizer(true); 
            };
            utterance.onend = () => { 
                isSpeaking = false; 
                toggleVisualizer(false); 
            };
            speechSynthesis.speak(utterance);
        }

        function stopSpeaking(){
            if(speechSynthesis.speaking){
                speechSynthesis.cancel();
                isSpeaking = false;
                toggleVisualizer(false);
            }
        }

        async function sendToServer(text){
            appendChatLine("user", text);
            const uid = getUserId();
            const profile = JSON.parse(localStorage.getItem("zyvora_profile") || "null");
            const payload = {user_id: uid, message: text, lang: profile?.language || "en"};
            appendChatLine("bot", "Thinking...");
            toggleVisualizer(true);
            const res = await fetch("/chat", {
                method:"POST", headers:{"Content-Type":"application/json"}, body: JSON.stringify(payload)
            }).then(r=>r.json());
            const box = document.getElementById("chatBox");
            box.removeChild(box.lastChild);
            if(res.error){
                appendChatLine("bot", "‚ö†Ô∏è " + res.error);
            } else {
                appendChatLine("bot", res.reply);
                speakText(res.reply, profile?.language || "en"); 
                if(res.profile) localStorage.setItem("zyvora_profile", JSON.stringify(res.profile));
            }
        }

        // --- Audio Visualizer & Recognition ---
        let recognition = null;
        const micIcon = document.getElementById("micIcon");
        const micContainer = document.getElementById("micContainer");
        const canvas = document.getElementById("visualizer");
        const ctx = canvas.getContext("2d");
        const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
        let analyser, dataArray, source;
        const barWidth = 4, barGap = 2;

        function toggleVisualizer(active) {
            if (active) {
                micIcon.classList.add("mic-pulse");
                micContainer.style.transform = "scale(1.1)";
            } else {
                micIcon.classList.remove("mic-pulse");
                micContainer.style.transform = "scale(1)";
                ctx.clearRect(0, 0, canvas.width, canvas.height);
            }
        }

        function drawVisualizer() {
            if (!analyser) return;
            analyser.getByteFrequencyData(dataArray);
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            ctx.fillStyle = 'rgba(0, 0, 0, 0.1)';
            ctx.fillRect(0, 0, canvas.width, canvas.height);

            const numBars = Math.floor(canvas.width / (barWidth + barGap));
            let x = (canvas.width - (numBars * (barWidth + barGap))) / 2;

            for (let i = 0; i < numBars; i++) {
                const barHeight = (dataArray[i] / 255) * canvas.height;
                const r = 255 - barHeight;
                const g = barHeight + 50;
                const b = 50;
                ctx.fillStyle = `rgb(${r}, ${g}, ${b})`;
                ctx.fillRect(x, canvas.height - barHeight, barWidth, barHeight);
                x += barWidth + barGap;
            }
            requestAnimationFrame(drawVisualizer);
        }

        async function setupAudio() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                source = audioCtx.createMediaStreamSource(stream);
                analyser = audioCtx.createAnalyser();
                analyser.fftSize = 256;
                dataArray = new Uint8Array(analyser.frequencyBinCount);
                source.connect(analyser);
                drawVisualizer();
            } catch (e) {
                appendChatLine("bot", "‚ö†Ô∏è Please allow microphone access to use this feature.");
            }
        }

        function setupVoiceRecognition(lang = 'en') {
            const SR = window.SpeechRecognition || window.webkitSpeechRecognition;
            if (!SR) {
                appendChatLine("bot", "‚ùå Speech recognition is not supported in this browser. Try Chrome.");
                return;
            }
            recognition = new SR();
            recognition.lang = lang;
            recognition.interimResults = false;
            recognition.continuous = true;

            recognition.onresult = (evt) => {
                const text = evt.results[evt.results.length - 1][0].transcript.trim().toLowerCase();

                // ‚úÖ Always listen for stop
                if (text.includes("stop")) {
                    stopSpeaking();
                    appendChatLine("user", "üõë You said stop");
                    return;
                }

                // Ignore AI's own speech except stop
                if (isSpeaking) {
                    console.log("Ignored speech while Zyvora is talking.");
                    return;
                }

                if (text.includes("switch to text mode")) {
                    stopSpeaking();
                    speakText("Switching to text mode now.");
                    window.location.href = "/text";
                    return;
                }

                sendToServer(text);
            };

            recognition.onerror = (e) => {
                console.error("Recognition error:", e);
                recognition.stop();
                setTimeout(() => recognition.start(), 500);
            };

            recognition.onend = () => {
                setTimeout(() => recognition.start(), 500);
            };

            recognition.start();
        }

        window.onload = async () => {
            await setupAudio();
            let profile = JSON.parse(localStorage.getItem("zyvora_profile") || "null");
            if (!profile) {
                const lang = prompt("Enter preferred language code (e.g., en for English, hi for Hindi):");
                profile = { user_id: getUserId(), language: lang || 'en' };
                localStorage.setItem("zyvora_profile", JSON.stringify(profile));
            }
            setupVoiceRecognition(profile.language);
        };

        document.getElementById("sendManual").addEventListener("click", () => {
            const t = document.getElementById("manualInput").value.trim();
            if (!t) return;
            document.getElementById("manualInput").value = "";
            sendToServer(t);
        });
    </script>
</body>
</html>
